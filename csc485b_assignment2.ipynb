{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[{"file_id":"1-qCROQ_PZi71nqLbcSx1iR2NKTUFpuQU","timestamp":1728854049388}],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Load the extension that allows us to compile CUDA code in python notebooks\n# Documentation is here: https://nvcc4jupyter.readthedocs.io/en/latest/\n!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n%load_ext nvcc4jupyter","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dclGnLGAgbtH","outputId":"6e7d90d2-c6a5-4d36-ac87-6d6921eb76a6","executionInfo":{"status":"ok","timestamp":1728851718513,"user_tz":420,"elapsed":7875,"user":{"displayName":"Dylan Stevens","userId":"11249102564501965714"}},"execution":{"iopub.status.busy":"2024-10-20T22:33:35.050800Z","iopub.execute_input":"2024-10-20T22:33:35.051712Z","iopub.status.idle":"2024-10-20T22:33:58.150292Z","shell.execute_reply.started":"2024-10-20T22:33:35.051673Z","shell.execute_reply":"2024-10-20T22:33:58.149148Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-kh9v01cw\n  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-kh9v01cw\n  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 28f872a2f99a1b201bcd0db14fdbc5a496b9bfd7\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hThe nvcc4jupyter extension is already loaded. To reload it, use:\n  %reload_ext nvcc4jupyter\n","output_type":"stream"}],"execution_count":84},{"cell_type":"code","source":"%%cuda_group_save -g \"source\" -n \"data_types.h\"\n/**\n * A collection of commonly used data types throughout this project.\n */\n#pragma once\n\n#include <iostream> // for std::ostream\n#include <vector>\n\nnamespace csc485b{\nnamespace a2{\n\nusing node_t = int;\nusing edge_t = int2;\n\nusing edge_list_t = std::vector< edge_t >;\nusing node_list_t = std::vector< node_t >;\n\n} // namespace a2\n} // namespace csc485b\n","metadata":{"id":"VVbDQthwogQF","executionInfo":{"status":"ok","timestamp":1728851718514,"user_tz":420,"elapsed":17,"user":{"displayName":"Dylan Stevens","userId":"11249102564501965714"}},"execution":{"iopub.status.busy":"2024-10-20T22:33:58.152647Z","iopub.execute_input":"2024-10-20T22:33:58.153180Z","iopub.status.idle":"2024-10-20T22:33:58.159160Z","shell.execute_reply.started":"2024-10-20T22:33:58.153110Z","shell.execute_reply":"2024-10-20T22:33:58.157887Z"},"trusted":true},"outputs":[],"execution_count":85},{"cell_type":"code","source":"%%cuda_group_save -g \"source\" -n \"cuda_common.h\"\n/**\n * Standard macros that can be useful for error checking.\n * https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__ERROR.html\n */\n#pragma once\n\n#include <cuda.h>\n\n#define CUDA_CALL(exp)                                       \\\n    do {                                                     \\\n        cudaError res = (exp);                               \\\n        if(res != cudaSuccess) {                             \\\n            printf(\"Error at %s:%d\\n %s\\n\",                  \\\n                __FILE__,__LINE__, cudaGetErrorString(res)); \\\n           exit(EXIT_FAILURE);                               \\\n        }                                                    \\\n    } while(0)\n\n#define CHECK_ERROR(msg)                                             \\\n    do {                                                             \\\n        cudaError_t err = cudaGetLastError();                        \\\n        if(cudaSuccess != err) {                                     \\\n            printf(\"Error (%s) at %s:%d\\n %s\\n\",                     \\\n                (msg), __FILE__, __LINE__, cudaGetErrorString(err)); \\\n            exit(EXIT_FAILURE);                                      \\\n        }                                                            \\\n    } while (0)","metadata":{"id":"ZqET4uI2ggwf","executionInfo":{"status":"ok","timestamp":1728851718514,"user_tz":420,"elapsed":16,"user":{"displayName":"Dylan Stevens","userId":"11249102564501965714"}},"execution":{"iopub.status.busy":"2024-10-20T22:33:58.160295Z","iopub.execute_input":"2024-10-20T22:33:58.160598Z","iopub.status.idle":"2024-10-20T22:33:58.171849Z","shell.execute_reply.started":"2024-10-20T22:33:58.160566Z","shell.execute_reply":"2024-10-20T22:33:58.171072Z"},"trusted":true},"outputs":[],"execution_count":86},{"cell_type":"code","source":"%%cuda_group_save -g \"source\" -n \"data_generator.h\"\n/**\n * Functions for generating random input data with a fixed seed\n */\n#pragma once\n\n#include <cassert>  // for assert()\n#include <cstddef>  // std::size_t type\n#include <random>   // for std::mt19937, std::uniform_int_distribution\n#include <vector>\n\n#include \"data_types.h\"\n\nnamespace csc485b {\nnamespace a2 {\n\n/**\n * Generates and returns a vector of random edges\n * for a graph `G=(V,E)` with `n=|V|=n` and expected `m=|E|`.\n * Referred to as an Erdős-Rényi graph.\n *\n * @see https://networkx.org/documentation/stable/reference/generated/networkx.generators.random_graphs.fast_gnp_random_graph.html#networkx.generators.random_graphs.fast_gnp_random_graph\n */\nedge_list_t generate_graph( std::size_t n, std::size_t m )\n{\n    assert( \"At most n(n-1) edges in a simple graph\" && m < n * ( n - 1 ) );\n\n    int const probability = ( 100 * m ) / ( n * ( n - 1 ) );\n\n    // for details of random number generation, see:\n    // https://en.cppreference.com/w/cpp/numeric/random/uniform_int_distribution\n    std::size_t random_seed = 20241008;  // use magic seed\n    std::mt19937 rng( random_seed );     // use mersenne twister generator\n    std::uniform_int_distribution<> distrib(0, 100);\n\n    edge_list_t random_edges;\n    random_edges.reserve( 2 * m );\n\n    for( node_t u = 0; u < n; ++u )\n    {\n        for( node_t v = u + 1; v < n; ++v )\n        {\n            auto const dice_roll = distrib( rng );\n            if( dice_roll <= probability )\n            {\n                random_edges.push_back( make_int2( u, v ) );\n                random_edges.push_back( make_int2( v, u ) );\n            }\n        }\n    }\n\n    random_edges.resize( random_edges.size() );\n\n\n    return random_edges;\n}\n\n} // namespace a2\n} // namespace csc485b\n","metadata":{"id":"GY0L7rKhoVaZ","executionInfo":{"status":"ok","timestamp":1728851718514,"user_tz":420,"elapsed":15,"user":{"displayName":"Dylan Stevens","userId":"11249102564501965714"}},"execution":{"iopub.status.busy":"2024-10-20T22:33:58.174224Z","iopub.execute_input":"2024-10-20T22:33:58.174893Z","iopub.status.idle":"2024-10-20T22:33:58.180687Z","shell.execute_reply.started":"2024-10-20T22:33:58.174824Z","shell.execute_reply":"2024-10-20T22:33:58.179809Z"},"trusted":true},"outputs":[],"execution_count":87},{"cell_type":"code","source":"%%cuda_group_save -g \"source\" -n \"dense_graph.h\"\n/**\n * The file in which you will implement your DenseGraph GPU solutions!\n */\n\n#include <cstddef>  // std::size_t type\n\n#include \"cuda_common.h\"\n#include \"data_types.h\"\n\nnamespace csc485b {\nnamespace a2      {\n\n/**\n * A DenseGraph is optimised for a graph in which the number of edges\n * is close to n(n-1). It is represented using an adjacency matrix.\n */\nstruct DenseGraph\n{\n  std::size_t n; /**< Number of nodes in the graph. */\n  node_t * adjacencyMatrix; /** Pointer to an n x n adj. matrix */\n\n  /** Returns number of cells in the adjacency matrix. */\n  __device__ __host__ __forceinline__\n  std::size_t matrix_size() const { return n * n; }\n};\n\nstruct DenseGraphCPU {\n    std::size_t n;                  // Number of nodes in the graph\n    std::vector<int> adjacencyMatrix; // Adjacency matrix stored as a 1D array\n\n    DenseGraphCPU(std::size_t nodes)\n        : n(nodes), adjacencyMatrix(nodes * nodes, 0) {}  // Initialize matrix with zeros\n\n    // Helper function to print the adjacency matrix\n    void print() const {\n        for (std::size_t i = 0; i < n * n; ++i) {\n            std::cout << adjacencyMatrix[i] << \" \";\n        }\n        std::cout << std::endl;\n    }\n};\n\nvoid build_graph_cpu(DenseGraphCPU& g, const edge_list_t& edge_list) {\n    for (const auto& edge : edge_list) {\n        int u = edge.x;  // Source node\n        int v = edge.y;  // Destination node\n\n        // Update adjacency matrix\n        g.adjacencyMatrix[u * g.n + v] = 1;  // Set g[u][v] = 1\n        g.adjacencyMatrix[v * g.n + u] = 1;  // Set g[v][u] = 1 (undirected)\n    }\n}\n\n\nnamespace gpu {\n\n#define TILE_SIZE 16\n\n/**\n * Constructs a DenseGraph from an input edge list of m edges.\n *\n * @pre The pointers in DenseGraph g have already been allocated.\n */\n__global__\nvoid build_graph(DenseGraph g, int2 const * edge_list, std::size_t m) {\n    extern __shared__ int shared_indxs[];  // Shared memory for adjacency matrix\n    \n    int lcl_id = threadIdx.x; //local thid 0, 1, 2 on first iter, 256, 257, 258 on second iter\n    int glb_id = blockIdx.x * blockDim.x + lcl_id; //global thid\n\n    // Load a portion of the adjacency matrix into shared memory\n    // We will load one row of the adjacency matrix at a time\n    //if (lcl_id < g.n * g.n) {\n    //    shared_adj_matrix[lcl_id] = g.adjacencyMatrix[lcl_id];\n    //}\n    \n    //__syncthreads();  // Ensure all threads have loaded the adjacency matrix\n    \n    // Ensure the edge processing only happens within bounds\n    if (glb_id < m) {\n        // Each thread processes one edge\n        int u = edge_list[glb_id].x;  // Source node\n        int v = edge_list[glb_id].y;  // Destination node\n\n        // Convert the 2D adjacency matrix indices into a 1D array\n        std::size_t u_idx = u * g.n + v; // Index for (u, v)\n        //std::size_t v_idx = v * g.n + u; // Index for (v, u), since the graph is undirected\n        \n        shared_indxs[lcl_id] = u_idx; \n        \n        __syncthreads();\n\n        g.adjacencyMatrix[shared_indxs[lcl_id]] = 1;\n    }\n}\n\n\n/**\n  * Repopulates the adjacency matrix as a new graph that represents\n  * the two-hop neighbourhood of input graph g\n  */\n__global__\nvoid two_hop_reachability(DenseGraph g) {\n    // Block and thread indices\n    int bx = blockIdx.x;   // Block row\n    int by = blockIdx.y;   // Block column\n    int tx = threadIdx.x;  // Thread x-index within the block\n    int ty = threadIdx.y;  // Thread y-index within the block\n\n    int row = by * TILE_SIZE + ty;\n    int col = bx * TILE_SIZE + tx;\n    \n    // Shared memory for tiles of A and B\n    __shared__ int As[TILE_SIZE][TILE_SIZE];\n    __shared__ int Bs[TILE_SIZE][TILE_SIZE];\n    \n    int n = g.n;\n\n    int Cvalue = 0;\n\n    // Loop over the tiles of the input matrices\n    for (int m = 0; m < (n + TILE_SIZE - 1) / TILE_SIZE; ++m) {\n        // Load tile from matrix A into shared memory\n        if (row < n && m * TILE_SIZE + tx < n)\n            As[ty][tx] = g.adjacencyMatrix[row * n + m * TILE_SIZE + tx];\n        else\n            As[ty][tx] = 0;\n\n        // Load tile from matrix B into shared memory\n        if (col < n && m * TILE_SIZE + ty < n)\n            Bs[ty][tx] = g.adjacencyMatrix[(m * TILE_SIZE + ty) * n + col];\n        else\n            Bs[ty][tx] = 0;\n\n        __syncthreads();\n\n        // Multiply the two tiles together\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            Cvalue += As[ty][k] * Bs[k][tx];\n        }\n\n        __syncthreads();\n    }\n\n    // Write the result back to the adjacency matrix\n    if (row < n && col < n) {\n        // Clamp the value to 0 or 1\n        int val = (Cvalue > 0) ? 1 : 0;\n\n        // Remove self-loops by setting the diagonal to zero\n        if (row == col)\n            val = 0;\n\n        g.adjacencyMatrix[row * n + col] = val;\n    }\n}\n\n} // namespace gpu\n} // namespace a2\n} // namespace csc485b","metadata":{"id":"bjTbQ3EO2NwQ","executionInfo":{"status":"ok","timestamp":1728851766306,"user_tz":420,"elapsed":213,"user":{"displayName":"Dylan Stevens","userId":"11249102564501965714"}},"execution":{"iopub.status.busy":"2024-10-20T22:33:58.182147Z","iopub.execute_input":"2024-10-20T22:33:58.182476Z","iopub.status.idle":"2024-10-20T22:33:58.193729Z","shell.execute_reply.started":"2024-10-20T22:33:58.182445Z","shell.execute_reply":"2024-10-20T22:33:58.192950Z"},"trusted":true},"outputs":[],"execution_count":88},{"cell_type":"code","source":"%%cuda_group_save -g \"source\" -n \"sparse_graph.h\"\n/**\n * The file in which you will implement your SparseGraph GPU solutions!\n */\n\n#include <cstddef>  // std::size_t type\n\n#include \"cuda_common.h\"\n#include \"data_types.h\"\n\nnamespace csc485b {\nnamespace a2      {\n\n/**\n * A SparseGraph is optimised for a graph in which the number of edges\n * is close to cn, for a small constanct c. It is represented in CSR format.\n */\nstruct SparseGraph\n{\n  std::size_t n; /**< Number of nodes in the graph. */\n  std::size_t m; /**< Number of edges in the graph. */\n  node_t * neighbours_start_at; /** Pointer to an n=|V| offset array */\n  node_t * neighbours; /** Pointer to an m=|E| array of edge destinations */\n};\n\n\nnamespace gpu {\n\n/**\n * Constructs a SparseGraph from an input edge list of m edges.\n *\n * @pre The pointers in SparseGraph g have already been allocated.\n */\n\n__global__\nvoid count_edges(SparseGraph g, edge_t const* edge_list, std::size_t m) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx >= m) return;\n\n    // Each thread processes one edge from the edge list\n    edge_t edge = edge_list[idx];\n    int from = edge.x;\n\n    // Atomic increment the count for the 'from' node\n    atomicAdd(&g.neighbours_start_at[from + 1], 1);\n}\n\n__global__\nvoid prefix_sum(SparseGraph g) {\n    int idx = threadIdx.x;\n    int n = g.n;\n\n    // Perform a scan to calculate the offsets for each node\n    for (int i = 1; i < n; i <<= 1) {\n        __syncthreads();  // Sync threads to ensure correctness\n\n        if (idx >= i) {\n            g.neighbours_start_at[idx] += g.neighbours_start_at[idx - i];\n        }\n    }\n}\n\n__global__\nvoid build_graph(SparseGraph g, edge_t const* edge_list, std::size_t m) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx >= m) return;\n\n    // Each thread processes one edge from the edge list\n    edge_t edge = edge_list[idx];\n    int from = edge.x;\n    int to = edge.y;\n\n    // Atomic increment the offset for the 'from' node\n    int pos = atomicAdd(&g.neighbours_start_at[from + 1], 1);\n    \n    // Add the neighbor\n    g.neighbours[pos] = to;\n}\n    \n    \n\n/**\n  * Repopulates the adjacency lists as a new graph that represents\n  * the two-hop neighbourhood of input graph g\n  */\n__global__\nvoid two_hop_reachability( SparseGraph g )\n{\n    // IMPLEMENT ME!\n    // algorithm unknown\n    return;\n}\n\n} // namespace gpu\n} // namespace a2\n} // namespace csc485b","metadata":{"id":"ChKZ0hEWwcLu","executionInfo":{"status":"ok","timestamp":1728851718515,"user_tz":420,"elapsed":15,"user":{"displayName":"Dylan Stevens","userId":"11249102564501965714"}},"execution":{"iopub.status.busy":"2024-10-20T22:33:58.195286Z","iopub.execute_input":"2024-10-20T22:33:58.195649Z","iopub.status.idle":"2024-10-20T22:33:58.205817Z","shell.execute_reply.started":"2024-10-20T22:33:58.195608Z","shell.execute_reply":"2024-10-20T22:33:58.204951Z"},"trusted":true},"outputs":[],"execution_count":89},{"cell_type":"code","source":"%%cuda_group_save -g \"source\" -n \"main.cu\"\n/**\n * Driver for the benchmark comparison. Generates random data,\n * runs the CPU baseline, and then runs your code.\n */\n\n#include <chrono>   // for timing\n#include <iostream> // std::cout, std::endl\n#include <iterator> // std::ostream_iterator\n#include <vector>\n\n#include \"dense_graph.h\"\n#include \"sparse_graph.h\"\n\n#include \"data_generator.h\"\n#include \"data_types.h\"\n\n/**\n * Runs timing tests on a CUDA graph implementation.\n * Consists of independently constructing the graph and then\n * modifying it to its two-hop neighbourhood.\n */\ntemplate < typename DeviceGraph >\nvoid run( DeviceGraph g, csc485b::a2::edge_t const * d_edges, std::size_t m )\n{\n    cudaDeviceSynchronize();\n    auto const build_start = std::chrono::high_resolution_clock::now();\n\n    int threadsPerBlock = 1024;\n    int blocks = (m + threadsPerBlock - 1) / threadsPerBlock;\n    size_t shared_mem_size = threadsPerBlock * sizeof(csc485b::a2::node_t);  // Size of shared memory\n    std::cout << \"Shared mem size: \" << shared_mem_size << std::endl;\n\n    csc485b::a2::gpu::build_graph<<<blocks, threadsPerBlock, shared_mem_size>>>(g, d_edges, m);\n    \n    \n    cudaDeviceSynchronize();\n    auto const reachability_start = std::chrono::high_resolution_clock::now();\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((g.n + TILE_SIZE - 1) / TILE_SIZE, (g.n + TILE_SIZE - 1) / TILE_SIZE);\n\n    csc485b::a2::gpu::two_hop_reachability<<<gridDim, blockDim>>>(g);\n\n    cudaDeviceSynchronize();\n    auto const end = std::chrono::high_resolution_clock::now();\n\n    std::cout << \"Build time: \"\n              << std::chrono::duration_cast<std::chrono::microseconds>(reachability_start - build_start).count()\n              << \" us\"\n              << std::endl;\n\n    std::cout << \"Reachability time: \"\n              << std::chrono::duration_cast<std::chrono::microseconds>(end - reachability_start).count()\n              << \" us\"\n              << std::endl;\n}\n\n/**\n * Allocates space for a dense graph and then runs the test code on it.\n */\nvoid run_dense( csc485b::a2::edge_t const * d_edges, std::size_t n, std::size_t m )\n{\n    using namespace csc485b;\n\n    // allocate device DenseGraph\n    a2::node_t * d_matrix;\n    cudaMalloc( (void**)&d_matrix, sizeof( a2::node_t ) * n * n );\n    a2::DenseGraph d_dg{ n, d_matrix };\n\n    run( d_dg, d_edges, m );\n\n    // check output?\n    std::vector< a2::node_t > host_matrix( d_dg.matrix_size() );\n    a2::DenseGraph dg{ n, host_matrix.data() };\n    cudaMemcpy( dg.adjacencyMatrix, d_dg.adjacencyMatrix, sizeof( a2::node_t ) * d_dg.matrix_size(), cudaMemcpyDeviceToHost );\n    std::copy( host_matrix.cbegin(), host_matrix.cend(), std::ostream_iterator< a2::node_t >( std::cout, \" \" ) );\n\n    // clean up\n    cudaFree( d_matrix );\n}\n\n/**\n * Allocates space for a sparse graph and then runs the test code on it.\n */\nvoid run_sparse(csc485b::a2::edge_t const* d_edges, std::size_t n, std::size_t m) {\n    using namespace csc485b;\n\n    // Allocate device SparseGraph\n    a2::node_t* d_offsets, *d_neighbours;\n    cudaMalloc((void**)&d_offsets, sizeof(a2::node_t) * (n + 1));  // +1 for the last offset\n    cudaMalloc((void**)&d_neighbours, sizeof(a2::node_t) * m);\n    a2::SparseGraph d_sg{ n, m, d_offsets, d_neighbours };\n\n    // Initialize neighbours_start_at on the device to zero\n    cudaMemset(d_offsets, 0, sizeof(a2::node_t) * (n + 1));\n\n    // Kernel for counting edges\n    int threadsPerBlock = 256;\n    int blocks = (m + threadsPerBlock - 1) / threadsPerBlock;\n    csc485b::a2::gpu::count_edges<<<blocks, threadsPerBlock>>>(d_sg, d_edges, m);\n\n    // Perform prefix sum to compute the offsets\n    csc485b::a2::gpu::prefix_sum<<<1, n>>>(d_sg);  // Launch with 1 block, each thread computes an entry in the prefix sum\n\n    // Now, launch the build_graph kernel\n    blocks = (m + threadsPerBlock - 1) / threadsPerBlock;\n    csc485b::a2::gpu::build_graph<<<blocks, threadsPerBlock>>>(d_sg, d_edges, m);  // Pass offset as needed\n\n    cudaDeviceSynchronize();\n\n    // Now retrieve the result from the device to the host\n    std::vector<a2::node_t> host_neighbours(m);\n    std::vector<a2::node_t> host_offsets(n + 1);\n    cudaMemcpy(host_offsets.data(), d_sg.neighbours_start_at, sizeof(a2::node_t) * (n + 1), cudaMemcpyDeviceToHost);\n    cudaMemcpy(host_neighbours.data(), d_sg.neighbours, sizeof(a2::node_t) * m, cudaMemcpyDeviceToHost);\n\n    // Output results\n    std::cout << \"\\n\\nneighbours: \";\n    std::copy(host_neighbours.begin(), host_neighbours.end(), std::ostream_iterator<a2::node_t>(std::cout, \" \"));\n    std::cout << \"\\nneighbours_start_at: \";\n    std::copy(host_offsets.begin(), host_offsets.end(), std::ostream_iterator<a2::node_t>(std::cout, \" \"));\n    \n    // Clean up\n    cudaFree(d_neighbours);\n    cudaFree(d_offsets);\n}\n\n\n\nint main()\n{\n    using namespace csc485b;\n\n    // Create input\n    std::size_t constexpr n = 16;\n    std::size_t constexpr expected_degree = n >> 1;\n\n    a2::edge_list_t const graph = a2::generate_graph( n, n * expected_degree );\n    std::size_t const m = graph.size();\n\n    // lazily echo out input graph\n    for( auto const& e : graph )\n    {\n        std::cout << \"(\" << e.x << \",\" << e.y << \") \";\n    }\n    std::cout << std::endl;\n\n    std::cout << m;\n    std::cout << std::endl;\n\n    // allocate and memcpy input to device\n    a2::edge_t * d_edges;\n    cudaMalloc( (void**)&d_edges, sizeof( a2::edge_t ) * m );\n    cudaMemcpyAsync( d_edges, graph.data(), sizeof( a2::edge_t ) * m, cudaMemcpyHostToDevice );\n\n    a2::DenseGraphCPU g_cpu(n);\n    build_graph_cpu(g_cpu, graph);\n    std::cout << \"CPU Adjacency Matrix:\" << std::endl;\n    g_cpu.print();\n    std::cout << std::endl;\n\n    // run your code!\n    run_dense ( d_edges, n, m );\n    std::cout << std::endl;\n\n    cudaDeviceProp prop;\n    cudaGetDeviceProperties(&prop, 0);\n    std::cout << \"Max shared memory per block: \" << prop.sharedMemPerBlock << \" bytes\" << std::endl;\n\n    //run_sparse( d_edges, n, m );\n\n    return EXIT_SUCCESS;\n}","metadata":{"id":"IRvVeK-QifnZ","executionInfo":{"status":"ok","timestamp":1728851718515,"user_tz":420,"elapsed":14,"user":{"displayName":"Dylan Stevens","userId":"11249102564501965714"}},"execution":{"iopub.status.busy":"2024-10-20T22:36:41.177998Z","iopub.execute_input":"2024-10-20T22:36:41.178664Z","iopub.status.idle":"2024-10-20T22:36:41.187455Z","shell.execute_reply.started":"2024-10-20T22:36:41.178623Z","shell.execute_reply":"2024-10-20T22:36:41.186517Z"},"trusted":true},"outputs":[],"execution_count":92},{"cell_type":"code","source":"%cuda_group_run --group \"source\" --compiler-args \"-O3 -g -std=c++20 -arch=sm_75\"","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S7F0eVsGjUNp","outputId":"eef1ecb6-ee8e-4f48-8a81-8cfcb0d88c6b","executionInfo":{"status":"ok","timestamp":1728851774373,"user_tz":420,"elapsed":5049,"user":{"displayName":"Dylan Stevens","userId":"11249102564501965714"}},"execution":{"iopub.status.busy":"2024-10-20T22:36:44.710356Z","iopub.execute_input":"2024-10-20T22:36:44.710758Z","iopub.status.idle":"2024-10-20T22:36:48.760789Z","shell.execute_reply.started":"2024-10-20T22:36:44.710726Z","shell.execute_reply":"2024-10-20T22:36:48.759882Z"},"trusted":true},"outputs":[{"name":"stdout","text":"(0,2) (2,0) (0,3) (3,0) (0,4) (4,0) (0,5) (5,0) (0,6) (6,0) (0,9) (9,0) (0,10) (10,0) (0,11) (11,0) (0,13) (13,0) (0,14) (14,0) (1,2) (2,1) (1,6) (6,1) (1,7) (7,1) (1,9) (9,1) (1,10) (10,1) (1,12) (12,1) (1,13) (13,1) (1,14) (14,1) (2,3) (3,2) (2,4) (4,2) (2,5) (5,2) (2,6) (6,2) (2,8) (8,2) (2,9) (9,2) (2,11) (11,2) (2,12) (12,2) (2,13) (13,2) (3,4) (4,3) (3,5) (5,3) (3,7) (7,3) (3,11) (11,3) (3,12) (12,3) (3,15) (15,3) (4,10) (10,4) (4,13) (13,4) (5,6) (6,5) (5,7) (7,5) (5,8) (8,5) (5,9) (9,5) (5,11) (11,5) (5,14) (14,5) (5,15) (15,5) (6,7) (7,6) (6,10) (10,6) (6,12) (12,6) (6,14) (14,6) (7,8) (8,7) (7,9) (9,7) (7,10) (10,7) (7,12) (12,7) (7,13) (13,7) (7,14) (14,7) (7,15) (15,7) (8,9) (9,8) (8,11) (11,8) (9,10) (10,9) (9,11) (11,9) (9,13) (13,9) (9,14) (14,9) (10,11) (11,10) (10,12) (12,10) (10,13) (13,10) (10,14) (14,10) (11,12) (12,11) (11,14) (14,11) (11,15) (15,11) (12,15) (15,12) (13,14) (14,13) (13,15) (15,13) (14,15) (15,14) \n140\nCPU Adjacency Matrix:\n0 0 1 1 1 1 1 0 0 1 1 1 0 1 1 0 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 0 0 1 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 \n\nShared mem size: 4096\nBuild time: 206 us\nReachability time: 23 us\n0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 \nMax shared memory per block: 49152 bytes\n\n","output_type":"stream"}],"execution_count":93},{"cell_type":"code","source":"","metadata":{"id":"K0Yqomwu6WsP","executionInfo":{"status":"ok","timestamp":1728851724457,"user_tz":420,"elapsed":21,"user":{"displayName":"Dylan Stevens","userId":"11249102564501965714"}},"trusted":true},"outputs":[],"execution_count":null}]}